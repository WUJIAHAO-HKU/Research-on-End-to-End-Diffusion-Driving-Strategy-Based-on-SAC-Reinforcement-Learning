"""
è¿è¡Œæ‰€æœ‰Baselineç®—æ³•çš„è¯„ä¼°

è¯„ä¼°ï¼š
1. BC (Behavior Cloning) 
2. SAC-Diffusion (æœ¬é¡¹ç›®)
3. MPC (ä½œä¸ºä¸Šç•Œå‚è€ƒ)
"""

import argparse
import sys
import json
from pathlib import Path
from datetime import datetime

from isaaclab.app import AppLauncher

# å‚æ•°è§£æ
parser = argparse.ArgumentParser(description="è¯„ä¼°æ‰€æœ‰Baselineç®—æ³•")
parser.add_argument("--num_envs", type=int, default=4, help="å¹¶è¡Œç¯å¢ƒæ•°é‡")
parser.add_argument("--num_episodes", type=int, default=30, help="æ¯ä¸ªç®—æ³•è¯„ä¼°çš„episodeæ•°")
parser.add_argument("--max_steps", type=int, default=500, help="æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°")
AppLauncher.add_app_launcher_args(parser)
args = parser.parse_args()

# å¼ºåˆ¶headlessæ¨¡å¼
args.headless = True
args.enable_cameras = True

# å¯åŠ¨Isaac Sim
app_launcher = AppLauncher(args)
simulation_app = app_launcher.app

# Isaac Labå¯¼å…¥
from rosorin_env_cfg import ROSOrinEnvCfg
from isaaclab.envs import ManagerBasedRLEnv
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm


# ===== æ¨¡å‹å®šä¹‰ =====
class BCPolicy(nn.Module):
    """BCç­–ç•¥ç½‘ç»œ"""
    def __init__(self, obs_dim, action_dim, hidden_dims=[512, 512, 256]):
        super().__init__()
        layers = []
        prev_dim = obs_dim
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, action_dim))
        self.network = nn.Sequential(*layers)
    
    def forward(self, obs):
        return self.network(obs)


class SACPolicy(nn.Module):
    """SACç­–ç•¥ç½‘ç»œï¼ˆåŒ¹é…è®­ç»ƒæ—¶çš„ç»“æ„ï¼‰"""
    def __init__(self, obs_dim, action_dim, hidden_dim=512):
        super().__init__()
        # è§‚æµ‹ç¼–ç å™¨: [512, 512, 256]
        self.obs_encoder = nn.Sequential(
            nn.Linear(obs_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
        )
        # åŠ¨ä½œå¤´: [256, action_dim] åŒ¹é…è®­ç»ƒcheckpoint
        self.action_head = nn.Sequential(
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
        )
    
    def forward(self, obs):
        features = self.obs_encoder(obs)
        return self.action_head(features)


def load_bc_model(checkpoint_path, device):
    """åŠ è½½BCæ¨¡å‹"""
    # ä¿®å¤numpyå…¼å®¹æ€§
    if not hasattr(np, '_core'):
        import numpy.core as _core
        sys.modules['numpy._core'] = _core
        sys.modules['numpy._core.multiarray'] = _core.multiarray
        sys.modules['numpy._core.umath'] = _core.umath
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    obs_dim = len(checkpoint['obs_mean'])
    action_dim = len(checkpoint['action_mean'])
    
    model = BCPolicy(obs_dim, action_dim).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    obs_mean = torch.from_numpy(np.array(checkpoint['obs_mean'])).to(device)
    obs_std = torch.from_numpy(np.array(checkpoint['obs_std'])).to(device)
    action_mean = torch.from_numpy(np.array(checkpoint['action_mean'])).to(device)
    action_std = torch.from_numpy(np.array(checkpoint['action_std'])).to(device)
    
    return model, obs_mean, obs_std, action_mean, action_std


def load_sac_model(checkpoint_path, device, obs_dim=76810, action_dim=4):
    """åŠ è½½SAC-Diffusionæ¨¡å‹"""
    # ä¿®å¤numpyå…¼å®¹æ€§
    if not hasattr(np, '_core'):
        import numpy.core as _core
        sys.modules['numpy._core'] = _core
        sys.modules['numpy._core.multiarray'] = _core.multiarray
        sys.modules['numpy._core.umath'] = _core.umath
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # SACæ¨¡å‹ä½¿ç”¨ä¸åŒçš„ç½‘ç»œç»“æ„
    model = SACPolicy(obs_dim, action_dim).to(device)
    model.load_state_dict(checkpoint['actor'])
    model.eval()
    
    # ä½¿ç”¨é›¶å‡å€¼å’Œå•ä½æ–¹å·®ï¼ˆå‡è®¾SACè®­ç»ƒæ—¶å·²ç»å¤„ç†äº†å½’ä¸€åŒ–ï¼‰
    obs_mean = torch.zeros(obs_dim).to(device)
    obs_std = torch.ones(obs_dim).to(device)
    action_mean = torch.zeros(action_dim).to(device)
    action_std = torch.ones(action_dim).to(device)
    
    return model, obs_mean, obs_std, action_mean, action_std


def evaluate_policy(env, model, obs_mean, obs_std, action_mean, action_std, 
                   num_episodes, max_steps, policy_name):
    """è¯„ä¼°ç­–ç•¥"""
    device = next(model.parameters()).device
    num_envs = env.num_envs
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    episodes_done = 0
    env_episode_rewards = [0.0] * num_envs
    env_episode_lengths = [0] * num_envs
    
    obs_dict, _ = env.reset()
    
    print(f"\n{'='*80}")
    print(f"  è¯„ä¼° {policy_name}")
    print(f"{'='*80}")
    
    pbar = tqdm(total=num_episodes, desc=f"{policy_name}")
    
    for step in range(max_steps * num_episodes // num_envs + max_steps):
        obs = obs_dict["policy"]
        obs = torch.nan_to_num(obs, nan=0.0, posinf=10.0, neginf=0.0)
        
        # å½’ä¸€åŒ–
        obs_normalized = (obs - obs_mean) / (obs_std + 1e-8)
        
        # é¢„æµ‹åŠ¨ä½œ
        with torch.no_grad():
            action_normalized = model(obs_normalized)
        
        # åå½’ä¸€åŒ–
        action = action_normalized * action_std + action_mean
        action = torch.clamp(action, -1.0, 1.0)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        obs_dict, rewards, dones, truncated, infos = env.step(action)
        
        # ç´¯ç§¯å¥–åŠ±
        for i in range(num_envs):
            env_episode_rewards[i] += rewards[i].item()
            env_episode_lengths[i] += 1
            
            if dones[i] or truncated[i]:
                episode_rewards.append(env_episode_rewards[i])
                episode_lengths.append(env_episode_lengths[i])
                
                if env_episode_rewards[i] > 5.0:
                    success_count += 1
                
                env_episode_rewards[i] = 0.0
                env_episode_lengths[i] = 0
                episodes_done += 1
                
                pbar.update(1)
                pbar.set_postfix({
                    'reward': f'{np.mean(episode_rewards):.2f}',
                    'success': f'{success_count}/{episodes_done}'
                })
                
                if episodes_done >= num_episodes:
                    break
        
        if episodes_done >= num_episodes:
            break
    
    pbar.close()
    
    results = {
        'policy': policy_name,
        'num_episodes': len(episode_rewards),
        'mean_reward': float(np.mean(episode_rewards)),
        'std_reward': float(np.std(episode_rewards)),
        'min_reward': float(np.min(episode_rewards)),
        'max_reward': float(np.max(episode_rewards)),
        'mean_length': float(np.mean(episode_lengths)),
        'std_length': float(np.std(episode_lengths)),
        'success_rate': float(success_count / len(episode_rewards)),
        'episode_rewards': [float(r) for r in episode_rewards],
        'episode_lengths': [int(l) for l in episode_lengths]
    }
    
    return results


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºç¯å¢ƒ
    print("\nåˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
    env_cfg = ROSOrinEnvCfg()
    env_cfg.scene.num_envs = args.num_envs
    env = ManagerBasedRLEnv(cfg=env_cfg)
    
    # ç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = Path(f"experiments/baseline_comparison/comparison_{timestamp}")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    all_results = []
    
    # ===== 1. è¯„ä¼°BCæ¨¡å‹ =====
    bc_checkpoint = "experiments/bc_training/bc_training_20251228_052241/best_model.pt"
    if Path(bc_checkpoint).exists():
        print(f"\nåŠ è½½BCæ¨¡å‹: {bc_checkpoint}")
        bc_model, obs_mean, obs_std, action_mean, action_std = load_bc_model(bc_checkpoint, device)
        
        bc_results = evaluate_policy(
            env, bc_model, obs_mean, obs_std, action_mean, action_std,
            args.num_episodes, args.max_steps, "BC (Behavior Cloning)"
        )
        all_results.append(bc_results)
        
        # ä¿å­˜BCç»“æœ
        with open(results_dir / "bc_results.json", 'w') as f:
            json.dump(bc_results, f, indent=2)
    else:
        print(f"âš  BCæ¨¡å‹ä¸å­˜åœ¨: {bc_checkpoint}")
    
    # ===== 2. è¯„ä¼°SAC-Diffusionæ¨¡å‹ =====
    sac_checkpoint = "experiments/sac_training/sac_training_20251228_062324/checkpoints/best_model.pt"
    if Path(sac_checkpoint).exists():
        print(f"\nåŠ è½½SAC-Diffusionæ¨¡å‹: {sac_checkpoint}")
        sac_model, obs_mean, obs_std, action_mean, action_std = load_sac_model(sac_checkpoint, device)
        
        sac_results = evaluate_policy(
            env, sac_model, obs_mean, obs_std, action_mean, action_std,
            args.num_episodes, args.max_steps, "SAC-Diffusion"
        )
        all_results.append(sac_results)
        
        # ä¿å­˜SACç»“æœ
        with open(results_dir / "sac_diffusion_results.json", 'w') as f:
            json.dump(sac_results, f, indent=2)
    else:
        print(f"âš  SAC-Diffusionæ¨¡å‹ä¸å­˜åœ¨: {sac_checkpoint}")
    
    # ===== ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š =====
    print(f"\n{'='*80}")
    print("  ğŸ“Š Baselineå¯¹æ¯”ç»“æœ")
    print(f"{'='*80}\n")
    
    print(f"{'ç®—æ³•':<20} {'å¹³å‡å¥–åŠ±':<15} {'æˆåŠŸç‡':<15} {'å¹³å‡é•¿åº¦':<15}")
    print("-" * 80)
    
    for result in all_results:
        print(f"{result['policy']:<20} "
              f"{result['mean_reward']:>7.2f} Â± {result['std_reward']:<5.2f} "
              f"{result['success_rate']:>6.1%}        "
              f"{result['mean_length']:>7.1f} Â± {result['std_length']:<5.1f}")
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary = {
        'timestamp': timestamp,
        'num_episodes': args.num_episodes,
        'results': all_results
    }
    
    with open(results_dir / "summary.json", 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
    print(f"{'='*80}\n")
    
    env.close()
    simulation_app.close()


if __name__ == "__main__":
    main()
