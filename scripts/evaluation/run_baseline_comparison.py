"""
Baselineç®—æ³•å¯¹æ¯”å®éªŒ

å¯¹æ¯”ä»¥ä¸‹ç®—æ³•ï¼š
1. MPC (Model Predictive Control) - ä¸“å®¶ç­–ç•¥
2. BC (Behavior Cloning) - é¢„è®­ç»ƒç­–ç•¥
3. TD3 (Twin Delayed DDPG) - ç¡®å®šæ€§ç­–ç•¥
4. SAC-Gaussian (æ ‡å‡†SAC) - é«˜æ–¯éšæœºç­–ç•¥
5. SAC-Diffusion (æœ¬é¡¹ç›®) - æ‰©æ•£éšæœºç­–ç•¥
"""

import argparse
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List

# å°è¯•å¯¼å…¥å¯è§†åŒ–åº“
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
    sns.set_style("whitegrid")
    plt.rcParams['figure.figsize'] = (12, 8)
    plt.rcParams['font.size'] = 12
except ImportError:
    PLOTTING_AVAILABLE = False
    print("âš  matplotlib/seabornæœªå®‰è£…ï¼Œå°†è·³è¿‡å›¾è¡¨ç”Ÿæˆ")

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("âš  pandasæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–è¾“å‡º")

def load_results(results_dir: Path) -> Dict:
    """åŠ è½½å„ä¸ªç®—æ³•çš„è¯„ä¼°ç»“æœ"""
    results = {}
    
    # æŸ¥æ‰¾æ‰€æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶
    for method_dir in results_dir.iterdir():
        if method_dir.is_dir():
            result_file = method_dir / "evaluation_results.json"
            if result_file.exists():
                with open(result_file, 'r') as f:
                    results[method_dir.name] = json.load(f)
    
    return results


def create_comparison_table(results: Dict):
    """åˆ›å»ºå¯¹æ¯”è¡¨æ ¼"""
    data = []
    
    for method, result in results.items():
        data.append({
            'Method': method,
            'Mean Reward': result.get('mean_reward', 0),
            'Std Reward': result.get('std_reward', 0),
            'Success Rate (%)': result.get('success_rate', 0) * 100,
            'Mean Episode Length': result.get('mean_length', 0),
            'Training Time (min)': result.get('training_time', 0) / 60,
        })
    
    if PANDAS_AVAILABLE:
        import pandas as pd
        df = pd.DataFrame(data)
        df = df.sort_values('Mean Reward', ascending=False)
        return df
    else:
        # ç®€å•æ’åº
        data.sort(key=lambda x: x['Mean Reward'], reverse=True)
        return data


def plot_reward_comparison(results: Dict, save_path: Path):
    """ç»˜åˆ¶å¥–åŠ±å¯¹æ¯”å›¾"""
    if not PLOTTING_AVAILABLE:
        print("âš  è·³è¿‡ç»˜å›¾ï¼ˆmatplotlibæœªå®‰è£…ï¼‰")
        return
    
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    methods = list(results.keys())
    rewards = [results[m].get('mean_reward', 0) for m in methods]
    stds = [results[m].get('std_reward', 0) for m in methods]
    
    colors = sns.color_palette("husl", len(methods))
    bars = ax.bar(methods, rewards, yerr=stds, capsize=5, color=colors, alpha=0.8)
    
    ax.set_ylabel('Average Episode Reward', fontsize=14)
    ax.set_title('Algorithm Performance Comparison', fontsize=16, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}',
                ha='center', va='bottom', fontsize=11)
    
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(save_path / 'reward_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜å¥–åŠ±å¯¹æ¯”å›¾: {save_path / 'reward_comparison.png'}")


def plot_success_rate(results: Dict, save_path: Path):
    """ç»˜åˆ¶æˆåŠŸç‡å¯¹æ¯”å›¾"""
    if not PLOTTING_AVAILABLE:
        return
    
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    methods = list(results.keys())
    success_rates = [results[m].get('success_rate', 0) * 100 for m in methods]
    
    colors = sns.color_palette("coolwarm", len(methods))
    bars = ax.barh(methods, success_rates, color=colors, alpha=0.8)
    
    ax.set_xlabel('Success Rate (%)', fontsize=14)
    ax.set_title('Success Rate Comparison', fontsize=16, fontweight='bold')
    ax.grid(axis='x', alpha=0.3)
    ax.set_xlim([0, 100])
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars:
        width = bar.get_width()
        ax.text(width, bar.get_y() + bar.get_height()/2.,
                f'{width:.1f}%',
                ha='left', va='center', fontsize=11, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path / 'success_rate_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜æˆåŠŸç‡å¯¹æ¯”å›¾: {save_path / 'success_rate_comparison.png'}")


def plot_training_efficiency(results: Dict, save_path: Path):
    """ç»˜åˆ¶è®­ç»ƒæ•ˆç‡å¯¹æ¯”å›¾"""
    if not PLOTTING_AVAILABLE:
        return
    
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    # è¿‡æ»¤æ‰MPCï¼ˆä¸éœ€è¦è®­ç»ƒï¼‰
    trainable_methods = {k: v for k, v in results.items() 
                         if k != 'MPC' and 'training_time' in v}
    
    if not trainable_methods:
        print("âš  æ²¡æœ‰è®­ç»ƒæ—¶é—´æ•°æ®ï¼Œè·³è¿‡è®­ç»ƒæ•ˆç‡å›¾")
        return
    
    methods = list(trainable_methods.keys())
    times = [trainable_methods[m]['training_time'] / 60 for m in methods]
    rewards = [trainable_methods[m].get('mean_reward', 0) for m in methods]
    
    colors = sns.color_palette("Set2", len(methods))
    
    for i, method in enumerate(methods):
        ax.scatter(times[i], rewards[i], s=200, c=[colors[i]], 
                  alpha=0.7, edgecolors='black', linewidth=2)
        ax.annotate(method, (times[i], rewards[i]), 
                   xytext=(10, 10), textcoords='offset points',
                   fontsize=11, fontweight='bold')
    
    ax.set_xlabel('Training Time (minutes)', fontsize=14)
    ax.set_ylabel('Final Reward', fontsize=14)
    ax.set_title('Training Efficiency: Reward vs Time', fontsize=16, fontweight='bold')
    ax.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path / 'training_efficiency.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜è®­ç»ƒæ•ˆç‡å›¾: {save_path / 'training_efficiency.png'}")


def generate_latex_table(df, save_path: Path):
    """ç”ŸæˆLaTeXæ ¼å¼çš„è¡¨æ ¼"""
    if not PANDAS_AVAILABLE:
        print("âš  pandasæœªå®‰è£…ï¼Œè·³è¿‡LaTeXè¡¨æ ¼ç”Ÿæˆ")
        return
    
    latex_str = df.to_latex(
        index=False,
        float_format="%.2f",
        caption="Algorithm Performance Comparison",
        label="tab:comparison"
    )
    
    latex_file = save_path / 'comparison_table.tex'
    with open(latex_file, 'w') as f:
        f.write(latex_str)
    
    print(f"âœ“ ä¿å­˜LaTeXè¡¨æ ¼: {latex_file}")


def generate_report(results: Dict, save_path: Path):
    """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
    report = []
    report.append("=" * 80)
    report.append("  Baseline Algorithm Comparison Report")
    report.append("=" * 80)
    report.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    df = create_comparison_table(results)
    report.append("\n## æ€§èƒ½å¯¹æ¯”è¡¨æ ¼\n")
    
    if PANDAS_AVAILABLE:
        report.append(df.to_string(index=False))
        best_method = df.iloc[0]['Method']
        best_reward = df.iloc[0]['Mean Reward']
        df_success = df.sort_values('Success Rate (%)', ascending=False)
        best_success = df_success.iloc[0]
    else:
        # ç®€å•æ ¼å¼åŒ–è¾“å‡º
        for row in df:
            report.append(f"{row['Method']:20s} | "
                         f"Reward: {row['Mean Reward']:6.2f} Â± {row['Std Reward']:5.2f} | "
                         f"Success: {row['Success Rate (%)']:5.1f}% | "
                         f"Length: {row['Mean Episode Length']:7.1f} | "
                         f"Time: {row['Training Time (min)']:6.1f}min")
        best_method = df[0]['Method']
        best_reward = df[0]['Mean Reward']
        best_success = max(df, key=lambda x: x['Success Rate (%)'])
    
    report.append(f"\n\n## å…³é”®å‘ç°\n")
    report.append(f"ğŸ† æœ€ä½³ç®—æ³•: {best_method} (å¹³å‡å¥–åŠ±: {best_reward:.2f})")
    
    # æˆåŠŸç‡æ’å
    report.append(f"âœ… æœ€é«˜æˆåŠŸç‡: {best_success['Method']} ({best_success['Success Rate (%)']:.1f}%)")
    
    # è®­ç»ƒæ•ˆç‡
    if PANDAS_AVAILABLE:
        df_efficient = df[df['Training Time (min)'] > 0].copy()
    else:
        df_efficient = [x for x in df if x['Training Time (min)'] > 0]
    
    if df_efficient is not None and len(df_efficient) > 0:
        if PANDAS_AVAILABLE:
            df_efficient['Efficiency'] = df_efficient['Mean Reward'] / df_efficient['Training Time (min)']
            df_efficient = df_efficient.sort_values('Efficiency', ascending=False)
            most_efficient = df_efficient.iloc[0]
            report.append(f"âš¡ è®­ç»ƒæ•ˆç‡æœ€é«˜: {most_efficient['Method']} "
                         f"(å¥–åŠ±/åˆ†é’Ÿ: {most_efficient['Efficiency']:.3f})")
        else:
            for row in df_efficient:
                row['Efficiency'] = row['Mean Reward'] / row['Training Time (min)']
            most_efficient = max(df_efficient, key=lambda x: x['Efficiency'])
            report.append(f"âš¡ è®­ç»ƒæ•ˆç‡æœ€é«˜: {most_efficient['Method']} "
                         f"(å¥–åŠ±/åˆ†é’Ÿ: {most_efficient['Efficiency']:.3f})")
    
    report.append("\n" + "=" * 80)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = save_path / 'comparison_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    print(f"\nâœ“ ä¿å­˜å®éªŒæŠ¥å‘Š: {report_file}\n")
    
    # æ‰“å°åˆ°æ§åˆ¶å°
    print('\n'.join(report))


def main():
    parser = argparse.ArgumentParser(description="Baselineç®—æ³•å¯¹æ¯”åˆ†æ")
    parser.add_argument("--results_dir", type=str, 
                       default="experiments/baseline_comparison",
                       help="ç»“æœç›®å½•")
    parser.add_argument("--output_dir", type=str,
                       default="experiments/comparison_results",
                       help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()
    
    results_dir = Path(args.results_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("  Baselineç®—æ³•å¯¹æ¯”åˆ†æ")
    print("=" * 80)
    print(f"ç»“æœç›®å½•: {results_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}\n")
    
    # å¦‚æœæ²¡æœ‰ç»“æœï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®
    if not results_dir.exists() or not any(results_dir.iterdir()):
        print("âš  æœªæ‰¾åˆ°è¯„ä¼°ç»“æœï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•...")
        create_example_results(results_dir)
    
    # åŠ è½½ç»“æœ
    print("åŠ è½½è¯„ä¼°ç»“æœ...")
    results = load_results(results_dir)
    
    if not results:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•è¯„ä¼°ç»“æœ")
        print(f"\nè¯·ç¡®ä¿åœ¨ {results_dir} ç›®å½•ä¸‹æœ‰ä»¥ä¸‹ç»“æ„:")
        print("  baseline_comparison/")
        print("    â”œâ”€â”€ MPC/evaluation_results.json")
        print("    â”œâ”€â”€ BC/evaluation_results.json")
        print("    â”œâ”€â”€ TD3/evaluation_results.json")
        print("    â”œâ”€â”€ SAC-Gaussian/evaluation_results.json")
        print("    â””â”€â”€ SAC-Diffusion/evaluation_results.json")
        return
    
    print(f"âœ“ åŠ è½½äº† {len(results)} ä¸ªç®—æ³•çš„ç»“æœ\n")
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    df = create_comparison_table(results)
    
    # ä¿å­˜è¡¨æ ¼
    csv_file = output_dir / 'comparison_results.csv'
    if PANDAS_AVAILABLE:
        df.to_csv(csv_file, index=False)
        print(f"âœ“ ä¿å­˜CSVè¡¨æ ¼: {csv_file}")
    else:
        # æ‰‹åŠ¨ä¿å­˜CSV
        with open(csv_file, 'w') as f:
            if df:
                keys = df[0].keys()
                f.write(','.join(keys) + '\n')
                for row in df:
                    f.write(','.join(str(row[k]) for k in keys) + '\n')
        print(f"âœ“ ä¿å­˜CSVè¡¨æ ¼: {csv_file}")
    
    # ç”Ÿæˆå›¾è¡¨
    print("\nç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    plot_reward_comparison(results, output_dir)
    plot_success_rate(results, output_dir)
    plot_training_efficiency(results, output_dir)
    
    # ç”ŸæˆLaTeXè¡¨æ ¼
    generate_latex_table(df, output_dir)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(results, output_dir)
    
    print("\n" + "=" * 80)
    print("âœ… å¯¹æ¯”åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print(f"\næŸ¥çœ‹ç»“æœ: {output_dir}")
    print("  - comparison_results.csv: æ•°æ®è¡¨æ ¼")
    print("  - reward_comparison.png: å¥–åŠ±å¯¹æ¯”å›¾")
    print("  - success_rate_comparison.png: æˆåŠŸç‡å¯¹æ¯”å›¾")
    print("  - training_efficiency.png: è®­ç»ƒæ•ˆç‡å›¾")
    print("  - comparison_report.txt: å®éªŒæŠ¥å‘Š")


def create_example_results(results_dir: Path):
    """åˆ›å»ºç¤ºä¾‹ç»“æœæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    example_results = {
        "MPC": {
            "mean_reward": 8.5,
            "std_reward": 0.3,
            "success_rate": 0.95,
            "mean_length": 450,
        },
        "BC": {
            "mean_reward": 3.0,
            "std_reward": 0.5,
            "success_rate": 0.20,
            "mean_length": 1500,
            "training_time": 1200,  # 20åˆ†é’Ÿ
        },
        "TD3": {
            "mean_reward": 5.2,
            "std_reward": 1.2,
            "success_rate": 0.45,
            "mean_length": 850,
            "training_time": 3600,  # 60åˆ†é’Ÿ
        },
        "SAC-Gaussian": {
            "mean_reward": 6.1,
            "std_reward": 0.9,
            "success_rate": 0.55,
            "mean_length": 720,
            "training_time": 3300,  # 55åˆ†é’Ÿ
        },
        "SAC-Diffusion": {
            "mean_reward": 3.0,
            "std_reward": 0.0,
            "success_rate": 0.20,
            "mean_length": 1500,
            "training_time": 832,  # 14åˆ†é’Ÿ
        }
    }
    
    for method, data in example_results.items():
        method_dir = results_dir / method
        method_dir.mkdir(parents=True, exist_ok=True)
        
        result_file = method_dir / "evaluation_results.json"
        with open(result_file, 'w') as f:
            json.dump(data, f, indent=2)
    
    print(f"âœ“ åˆ›å»ºç¤ºä¾‹æ•°æ®: {results_dir}\n")


if __name__ == "__main__":
    main()
