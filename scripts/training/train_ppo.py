#!/usr/bin/env python3
"""
PPOè®­ç»ƒè„šæœ¬ (Proximal Policy Optimization)

è¯¥æ¨¡å—å®ç°PPOç®—æ³•çš„è®­ç»ƒæµç¨‹ï¼Œä½¿ç”¨PPOä¸“ç”¨çš„å¥–åŠ±é…ç½®ã€‚

ä¸»è¦åŠŸèƒ½:
- æ ‡å‡†PPOç®—æ³•è®­ç»ƒ
- è§‚å¯Ÿå€¼å½’ä¸€åŒ–ï¼ˆCPUç»Ÿè®¡ï¼ŒGPUå°æ‰¹é‡å½’ä¸€åŒ–ï¼‰
- å¥–åŠ±ç»„ä»¶æå–å’Œæ—¥å¿—è®°å½•
- æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜

ä½¿ç”¨æ–¹æ³•:
  ./isaaclab_runner.sh scripts/training/train_ppo.py --num_envs 8 --total_steps 100000
"""

import argparse
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import yaml
import json
import sys
import pickle

from isaaclab.app import AppLauncher

# è§£æå‚æ•°
parser = argparse.ArgumentParser(description="PPOè®­ç»ƒ")
parser.add_argument("--num_envs", type=int, default=8, help="å¹¶è¡Œç¯å¢ƒæ•°é‡ï¼ˆPPOé€‚åˆæ›´å¤šç¯å¢ƒï¼‰")
parser.add_argument("--total_steps", type=int, default=100000, help="æ€»è®­ç»ƒæ­¥æ•°")
parser.add_argument("--pretrain_checkpoint", type=str, default=None, help="BCé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆç”¨äºæ¶ˆèå®éªŒï¼‰")
parser.add_argument("--output_dir", type=str, default="experiments/baselines/ppo", help="è¾“å‡ºç›®å½•")
parser.add_argument("--batch_size", type=int, default=512, help="æ‰¹æ¬¡å¤§å°")
parser.add_argument("--n_steps", type=int, default=1024, help="æ¯æ¬¡rolloutæ­¥æ•°ï¼ˆé™ä½åˆ°1024èŠ‚çœæ˜¾å­˜ï¼‰")
parser.add_argument("--n_epochs", type=int, default=10, help="æ¯æ¬¡æ›´æ–°çš„epochæ•°")
parser.add_argument("--save_freq", type=int, default=10000, help="ä¿å­˜é¢‘ç‡")
parser.add_argument("--log_freq", type=int, default=100, help="æ—¥å¿—é¢‘ç‡")
parser.add_argument("--lr", type=float, default=3e-5, help="å­¦ä¹ ç‡ï¼ˆé™ä½åˆ°3e-5é¿å…NaNï¼‰")
parser.add_argument("--clip_range", type=float, default=0.2, help="PPO clipèŒƒå›´")
parser.add_argument("--vf_coef", type=float, default=0.5, help="ä»·å€¼å‡½æ•°ç³»æ•°")
parser.add_argument("--ent_coef", type=float, default=0.01, help="ç†µç³»æ•°")
AppLauncher.add_app_launcher_args(parser)
args = parser.parse_args()

# è®¾ç½®headlesså’Œç›¸æœº
args.headless = True
args.enable_cameras = True

# å¯åŠ¨Isaac Sim
app_launcher = AppLauncher(args)
simulation_app = app_launcher.app

# å¯¼å…¥Isaac Labå’Œè‡ªå®šä¹‰æ¨¡å—
# æ·»åŠ çˆ¶ç›®å½•åˆ°pathä»¥å¯¼å…¥env_factory
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from env_factory import create_ppo_env_cfg
from isaaclab.envs import ManagerBasedRLEnv
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal


print("="*80)
print("  PPOé©¾é©¶ç­–ç•¥è®­ç»ƒ")
print("="*80)
print(f"  å¹¶è¡Œç¯å¢ƒ: {args.num_envs}")
print(f"  æ€»æ­¥æ•°: {args.total_steps:,}")
print(f"  Rolloutæ­¥æ•°: {args.n_steps}")
print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
if args.pretrain_checkpoint:
    print(f"  BCé¢„è®­ç»ƒ: {args.pretrain_checkpoint}")
print("="*80)


# ============================================================================
# PPOç½‘ç»œå®šä¹‰
# ============================================================================

LOG_SIG_MAX = 2
LOG_SIG_MIN = -20
EPSILON = 1e-6


class _ImageEncoder(nn.Module):
    """è½»é‡CNNç¼–ç å™¨ï¼šæŠŠ(H,W,C)å‹æˆå›ºå®šç»´åº¦embeddingã€‚"""

    def __init__(self, in_channels: int, height: int, width: int, embed_dim: int = 64):
        super().__init__()
        self.height = int(height)
        self.width = int(width)
        self.in_channels = int(in_channels)

        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, 16, kernel_size=5, stride=2, padding=2),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
        )

        # åŠ¨æ€æ¨æ–­flattenç»´åº¦ï¼Œé¿å…ç¡¬ç¼–ç 
        with torch.no_grad():
            dummy = torch.zeros(1, in_channels, self.height, self.width)
            out = self.conv(dummy)
            flat_dim = int(out.reshape(1, -1).shape[1])

        self.head = nn.Sequential(
            nn.Linear(flat_dim, embed_dim),
            nn.ReLU(),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv(x)
        x = x.reshape(x.shape[0], -1)
        return self.head(x)


class ActorCritic(nn.Module):
    """Actor-Criticç½‘ç»œï¼ˆPPOï¼‰- Depth/RGB CNN encoder + ä½ç»´èåˆ"""

    def __init__(
        self,
        obs_dim: int,
        action_dim: int,
        low_dim: int,
        rgb_shape: tuple[int, int, int],
        depth_shape: tuple[int, int, int],
        hidden_dims=(256, 256),
        rgb_embed_dim: int = 64,
        depth_embed_dim: int = 64,
    ):
        super().__init__()

        self.obs_dim = int(obs_dim)
        self.action_dim = int(action_dim)
        self.low_dim = int(low_dim)

        rgb_h, rgb_w, rgb_c = rgb_shape
        dep_h, dep_w, dep_c = depth_shape
        self.rgb_h, self.rgb_w, self.rgb_c = int(rgb_h), int(rgb_w), int(rgb_c)
        self.dep_h, self.dep_w, self.dep_c = int(dep_h), int(dep_w), int(dep_c)

        self.rgb_dim = self.rgb_h * self.rgb_w * self.rgb_c
        self.depth_dim = self.dep_h * self.dep_w * self.dep_c

        if self.low_dim + self.rgb_dim + self.depth_dim != self.obs_dim:
            raise ValueError(
                f"obs split mismatch: low({self.low_dim}) + rgb({self.rgb_dim}) + depth({self.depth_dim}) != obs_dim({self.obs_dim})"
            )

        # ç¼–ç å™¨ï¼ˆè‡³å°‘depthåšCNNç¼–ç ï¼›è¿™é‡Œä¹ŸåŒæ—¶å¯¹RGBåšè½»é‡CNNï¼Œé¿å…ä¸¢ä¿¡æ¯ï¼‰
        self.rgb_encoder = _ImageEncoder(self.rgb_c, self.rgb_h, self.rgb_w, embed_dim=rgb_embed_dim)
        self.depth_encoder = _ImageEncoder(self.dep_c, self.dep_h, self.dep_w, embed_dim=depth_embed_dim)

        fusion_in_dim = self.low_dim + rgb_embed_dim + depth_embed_dim

        layers = []
        in_dim = fusion_in_dim
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.ReLU(),
            ])
            in_dim = hidden_dim
        self.shared = nn.Sequential(*layers)

        # Actorå¤´ï¼ˆç­–ç•¥ï¼‰
        self.actor_mean = nn.Linear(in_dim, action_dim)
        self.actor_log_std = nn.Parameter(torch.zeros(1, action_dim))

        # Criticå¤´ï¼ˆä»·å€¼å‡½æ•°ï¼‰
        self.critic = nn.Sequential(
            nn.Linear(in_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, obs):
        """å‰å‘ä¼ æ’­"""
        low = obs[:, : self.low_dim]
        rgb_flat = obs[:, self.low_dim : self.low_dim + self.rgb_dim]
        depth_flat = obs[:, self.low_dim + self.rgb_dim :]

        rgb = rgb_flat.reshape(obs.shape[0], self.rgb_h, self.rgb_w, self.rgb_c).permute(0, 3, 1, 2).contiguous()
        depth = depth_flat.reshape(obs.shape[0], self.dep_h, self.dep_w, self.dep_c).permute(0, 3, 1, 2).contiguous()

        rgb_feat = self.rgb_encoder(rgb)
        depth_feat = self.depth_encoder(depth)

        fused = torch.cat([low, rgb_feat, depth_feat], dim=-1)
        features = self.shared(fused)
        
        # Actor
        action_mean = self.actor_mean(features)
        action_log_std = self.actor_log_std.expand_as(action_mean)
        action_log_std = torch.clamp(action_log_std, LOG_SIG_MIN, LOG_SIG_MAX)
        action_std = action_log_std.exp()
        
        # Critic
        value = self.critic(features)
        
        return action_mean, action_std, value
    
    def get_action(self, obs, deterministic=False):
        """è·å–åŠ¨ä½œ"""
        action_mean, action_std, value = self.forward(obs)
        
        if deterministic:
            action = torch.tanh(action_mean)
        else:
            dist = Normal(action_mean, action_std)
            action_pre_tanh = dist.sample()
            action = torch.tanh(action_pre_tanh)
        
        return action, value
    
    def evaluate_actions(self, obs, actions):
        """è¯„ä¼°åŠ¨ä½œï¼ˆç”¨äºæ›´æ–°ï¼‰"""
        action_mean, action_std, value = self.forward(obs)
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(action_mean).any() or torch.isinf(action_mean).any():
            print(f"âš ï¸ è­¦å‘Š: action_meanåŒ…å«NaN/Inf")
            action_mean = torch.nan_to_num(action_mean, nan=0.0, posinf=1.0, neginf=-1.0)
        
        if torch.isnan(action_std).any() or torch.isinf(action_std).any():
            print(f"âš ï¸ è­¦å‘Š: action_stdåŒ…å«NaN/Inf")
            action_std = torch.nan_to_num(action_std, nan=0.1, posinf=1.0, neginf=0.01)
        
        # é™åˆ¶action_stdæœ€å°å€¼é¿å…æ•°å€¼é—®é¢˜
        action_std = torch.clamp(action_std, min=1e-3, max=2.0)
        
        dist = Normal(action_mean, action_std)
        
        # é€†tanh
        actions_pre_tanh = torch.atanh(torch.clamp(actions, -0.999, 0.999))
        
        # Logæ¦‚ç‡
        log_prob = dist.log_prob(actions_pre_tanh)
        # Tanhä¿®æ­£
        log_prob = log_prob - torch.log(1 - actions.pow(2) + EPSILON)
        log_prob = log_prob.sum(dim=-1, keepdim=True)
        
        # ç†µ
        entropy = dist.entropy().sum(dim=-1, keepdim=True)
        
        return value, log_prob, entropy


class RolloutBuffer:
    """PPO Rollout Buffer"""
    
    def __init__(self, buffer_size, obs_dim, action_dim, num_envs, device):
        self.buffer_size = buffer_size
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.num_envs = num_envs
        self.device = device
        
        self.reset()
    
    def reset(self):
        """é‡ç½®buffer"""
        self.observations = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
        self.pos = 0
    
    def add(self, obs, action, reward, value, log_prob, done):
        """æ·»åŠ ç»éªŒ"""
        self.observations.append(obs.cpu())
        self.actions.append(action.cpu())
        self.rewards.append(reward.cpu())
        self.values.append(value.cpu())
        self.log_probs.append(log_prob.cpu())
        self.dones.append(done.cpu())
        self.pos += 1
    
    def compute_returns_and_advantages(self, last_values, gamma=0.99, gae_lambda=0.95):
        """è®¡ç®—GAEä¼˜åŠ¿å’Œå›æŠ¥"""
        # è½¬æ¢ä¸ºtensor
        rewards = torch.stack(self.rewards)  # [n_steps, n_envs, 1]
        values = torch.stack(self.values)    # [n_steps, n_envs, 1]
        dones = torch.stack(self.dones)      # [n_steps, n_envs, 1]
        # ç¡®ä¿donesæ˜¯float maskï¼ˆ0.0/1.0ï¼‰ï¼Œé¿å…boolå‚ä¸ç®—æœ¯å¯¼è‡´RuntimeError
        if dones.dtype == torch.bool:
            dones = dones.float()
        
        advantages = torch.zeros_like(rewards)
        last_gae_lam = 0
        
        # ä»åå‘å‰è®¡ç®—GAE
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_values = last_values.cpu()
            else:
                next_values = values[t + 1]
            
            delta = rewards[t] + gamma * next_values * (1 - dones[t]) - values[t]
            advantages[t] = last_gae_lam = delta + gamma * gae_lambda * (1 - dones[t]) * last_gae_lam
        
        returns = advantages + values
        
        return returns, advantages
    
    def get(self, returns, advantages):
        """è·å–æ‰€æœ‰æ•°æ®"""
        # å±•å¹³batch
        observations = torch.stack(self.observations).reshape(-1, self.obs_dim)
        actions = torch.stack(self.actions).reshape(-1, self.action_dim)
        values = torch.stack(self.values).reshape(-1, 1)
        log_probs = torch.stack(self.log_probs).reshape(-1, 1)
        returns = returns.reshape(-1, 1)
        advantages = advantages.reshape(-1, 1)
        
        return observations, actions, values, log_probs, returns, advantages


class PPOAgent:
    """PPOæ™ºèƒ½ä½“"""
    
    def __init__(
        self,
        obs_dim,
        action_dim,
        device,
        low_dim=None,
        rgb_dim=None,
        depth_dim=None,
        rgb_shape=None,
        depth_shape=None,
        lr=3e-4,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        vf_coef=0.5,
        ent_coef=0.01,
        max_grad_norm=0.3,  # é™ä½åˆ°0.3
    ):
        self.device = device
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_range = clip_range
        self.vf_coef = vf_coef
        self.ent_coef = ent_coef
        self.max_grad_norm = max_grad_norm

        # è§‚æµ‹ç»“æ„ï¼ˆç”¨äºåªå½’ä¸€åŒ–ä½ç»´éƒ¨åˆ†ï¼Œé¿å…æŠŠåƒç´ ä¸€èµ·åšrunning mean/varå¯¼è‡´ä¿¡å·è¢«æ‰­æ›²ï¼‰
        self.low_dim = int(low_dim) if low_dim is not None else int(obs_dim)
        self.rgb_dim = int(rgb_dim) if rgb_dim is not None else 0
        self.depth_dim = int(depth_dim) if depth_dim is not None else 0
        
        # ğŸ†• è§‚æµ‹å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼ˆä¿æŒåœ¨CPUä¸ŠèŠ‚çœæ˜¾å­˜ï¼Œæ¯æ¬¡ä½¿ç”¨æ—¶ç§»åˆ°GPUï¼‰
        self.obs_mean = torch.zeros(self.low_dim)
        self.obs_var = torch.ones(self.low_dim)
        self.obs_count = 1e-4
        
        # Actor-Criticç½‘ç»œ
        if rgb_shape is None or depth_shape is None:
            raise ValueError("rgb_shape/depth_shape is required for CNN encoder policy")

        self.policy = ActorCritic(
            obs_dim=obs_dim,
            action_dim=action_dim,
            low_dim=self.low_dim,
            rgb_shape=rgb_shape,
            depth_shape=depth_shape,
        ).to(device)

        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
    
    def normalize_obs(self, obs):
        """å½’ä¸€åŒ–è§‚æµ‹ï¼ˆåœ¨çº¿æ›´æ–°ç»Ÿè®¡é‡ï¼‰"""
        if self.rgb_dim + self.depth_dim == 0 or self.low_dim == obs.shape[-1]:
            # å…¼å®¹è€è·¯å¾„ï¼šæ²¡æœ‰å›¾åƒåˆ‡åˆ†ä¿¡æ¯æ—¶ï¼Œå¯¹å…¨ç»´åšå½’ä¸€åŒ–
            obs_cpu = obs.cpu() if obs.is_cuda else obs

            batch_mean = obs_cpu.mean(dim=0)
            batch_var = obs_cpu.var(dim=0)
            batch_count = obs_cpu.shape[0]

            delta = batch_mean - self.obs_mean
            total_count = self.obs_count + batch_count

            self.obs_mean = self.obs_mean + delta * batch_count / total_count
            self.obs_var = (
                (self.obs_var * self.obs_count + batch_var * batch_count) / total_count +
                (delta ** 2) * self.obs_count * batch_count / (total_count ** 2)
            )
            self.obs_count = total_count

            obs_mean = self.obs_mean.to(obs.device)
            obs_std = torch.sqrt(self.obs_var).to(obs.device)

            obs_normalized = (obs - obs_mean) / (obs_std + 1e-8)
            obs_normalized = torch.clamp(obs_normalized, -10.0, 10.0)
            return obs_normalized

        # æ–°è·¯å¾„ï¼šåªå¯¹ä½ç»´çŠ¶æ€åšrunning mean/varï¼›å›¾åƒä¿æŒåŸå°ºåº¦ï¼ˆä»…åšclip/scaleï¼‰
        low = obs[:, : self.low_dim]
        rgb = obs[:, self.low_dim : self.low_dim + self.rgb_dim]
        depth = obs[:, self.low_dim + self.rgb_dim :]

        low_cpu = low.cpu() if low.is_cuda else low
        
        batch_mean = low_cpu.mean(dim=0)
        batch_var = low_cpu.var(dim=0)
        batch_count = low_cpu.shape[0]
        
        # å¢é‡æ›´æ–°
        delta = batch_mean - self.obs_mean
        total_count = self.obs_count + batch_count
        
        self.obs_mean = self.obs_mean + delta * batch_count / total_count
        self.obs_var = (
            (self.obs_var * self.obs_count + batch_var * batch_count) / total_count +
            (delta ** 2) * self.obs_count * batch_count / (total_count ** 2)
        )
        self.obs_count = total_count
        
        # å½’ä¸€åŒ–ï¼ˆå°†ç»Ÿè®¡é‡ä¸´æ—¶ç§»åˆ°obsæ‰€åœ¨è®¾å¤‡ï¼‰
        obs_mean = self.obs_mean.to(obs.device)
        obs_std = torch.sqrt(self.obs_var).to(obs.device)
        
        low_norm = (low - obs_mean) / (obs_std + 1e-8)
        low_norm = torch.clamp(low_norm, -10.0, 10.0)

        # RGB: é€šå¸¸æ˜¯0..255ï¼ˆæˆ–å·²å½’ä¸€åŒ–åˆ°0..1ï¼‰ï¼Œåšä¸€ä¸ªä¿å®ˆçš„è‡ªé€‚åº”scale
        if rgb.numel() > 0:
            rgb_max = rgb.detach().max().item()
            if rgb_max > 1.5:
                rgb = rgb / 255.0
            rgb = torch.clamp(rgb, 0.0, 1.0)

        # Depth: è£å‰ªåˆ°[0,10]å¹¶å½’ä¸€åŒ–åˆ°[0,1]
        if depth.numel() > 0:
            depth = torch.nan_to_num(depth, nan=10.0, posinf=10.0, neginf=0.0)
            depth = torch.clamp(depth, 0.0, 10.0) / 10.0

        return torch.cat([low_norm, rgb, depth], dim=-1)
    
    def select_action(self, obs, deterministic=False):
        """é€‰æ‹©åŠ¨ä½œ"""
        # ğŸ†• å½’ä¸€åŒ–è§‚æµ‹
        obs = self.normalize_obs(obs)
        
        with torch.no_grad():
            action, value = self.policy.get_action(obs, deterministic)
            
            if not deterministic:
                # è®¡ç®—log_probç”¨äºè®­ç»ƒ
                action_mean, action_std, _ = self.policy(obs)
                dist = Normal(action_mean, action_std)
                action_pre_tanh = torch.atanh(torch.clamp(action, -0.999, 0.999))
                log_prob = dist.log_prob(action_pre_tanh)
                log_prob = log_prob - torch.log(1 - action.pow(2) + EPSILON)
                log_prob = log_prob.sum(dim=-1, keepdim=True)
            else:
                log_prob = None
        
        return action, value, log_prob
    
    def update(self, rollout_buffer, n_epochs, batch_size):
        """æ›´æ–°ç­–ç•¥"""
        # è·å–æœ€åçš„valueç”¨äºGAE
        with torch.no_grad():
            last_obs = rollout_buffer.observations[-1].to(self.device)
            last_obs = self.normalize_obs(last_obs)  # ğŸ†• å½’ä¸€åŒ–
            _, _, last_values = self.policy(last_obs)
        
        # è®¡ç®—returnså’Œadvantages
        returns, advantages = rollout_buffer.compute_returns_and_advantages(
            last_values, self.gamma, self.gae_lambda
        )
        
        # è·å–æ•°æ®
        observations, actions, old_values, old_log_probs, returns, advantages = rollout_buffer.get(returns, advantages)
        
        # ğŸ†• åˆ†æ‰¹å½’ä¸€åŒ–è§‚æµ‹ï¼ˆé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
        # åªåœ¨CPUä¸Šæ›´æ–°ä½ç»´ç»Ÿè®¡é‡ï¼›å›¾åƒä¸å‚ä¸running mean/var
        obs_cpu = observations.cpu() if observations.is_cuda else observations
        low_cpu = obs_cpu[:, : self.low_dim]
        batch_mean = low_cpu.mean(dim=0)
        batch_var = low_cpu.var(dim=0)
        batch_count = low_cpu.shape[0]
        
        delta = batch_mean - self.obs_mean
        total_count = self.obs_count + batch_count
        self.obs_mean = self.obs_mean + delta * batch_count / total_count
        self.obs_var = (
            (self.obs_var * self.obs_count + batch_var * batch_count) / total_count +
            (delta ** 2) * self.obs_count * batch_count / (total_count ** 2)
        )
        self.obs_count = total_count
        
        # å½’ä¸€åŒ–å°†åœ¨æ¯ä¸ªmini-batchæ—¶è¿›è¡Œï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ•°æ®
        
        # æ ‡å‡†åŒ–advantagesï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        advantages = torch.clamp(advantages, -10.0, 10.0)  # é™åˆ¶èŒƒå›´
        
        # å‡†å¤‡æ•°æ®
        dataset_size = observations.shape[0]
        
        # è®­ç»ƒç»Ÿè®¡
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy = 0
        n_updates = 0
        
        for epoch in range(n_epochs):
            # éšæœºæ‰“ä¹±
            indices = torch.randperm(dataset_size)
            
            for start_idx in range(0, dataset_size, batch_size):
                end_idx = min(start_idx + batch_size, dataset_size)
                batch_indices = indices[start_idx:end_idx]
                
                batch_obs = observations[batch_indices].to(self.device)
                
                # ğŸ†• åœ¨mini-batchå±‚é¢å½’ä¸€åŒ–ï¼ˆé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
                # NOTE: ä»…å½’ä¸€åŒ–ä½ç»´çŠ¶æ€ï¼›å›¾åƒä¿æŒåŸå°ºåº¦ï¼ˆåœ¨normalize_obsä¸­åšclip/scaleï¼‰
                batch_obs = self.normalize_obs(batch_obs)
                
                batch_actions = actions[batch_indices].to(self.device)
                batch_old_log_probs = old_log_probs[batch_indices].to(self.device)
                batch_advantages = advantages[batch_indices].to(self.device)
                batch_returns = returns[batch_indices].to(self.device)
                
                # æ•°å€¼ç¨³å®šæ€§ï¼šæ£€æŸ¥è¾“å…¥
                if torch.isnan(batch_obs).any():
                    print(f"âš ï¸ è­¦å‘Š: batch_obsåŒ…å«NaNï¼Œè·³è¿‡æ­¤batch")
                    continue
                
                # è¯„ä¼°å½“å‰ç­–ç•¥
                values, log_probs, entropy = self.policy.evaluate_actions(batch_obs, batch_actions)
                
                # æ£€æŸ¥è¾“å‡º
                if torch.isnan(values).any() or torch.isnan(log_probs).any():
                    print(f"âš ï¸ è­¦å‘Š: valuesæˆ–log_probsåŒ…å«NaNï¼Œè·³è¿‡æ­¤batch")
                    continue
                
                # Policy loss (PPO clip)
                ratio = torch.exp(log_probs - batch_old_log_probs)
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_range, 1 + self.clip_range) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # Value loss
                value_loss = F.mse_loss(values, batch_returns)
                
                # Entropy loss
                entropy_loss = -entropy.mean()
                
                # Total loss
                loss = policy_loss + self.vf_coef * value_loss + self.ent_coef * entropy_loss
                
                # æ›´æ–°
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
                self.optimizer.step()
                
                # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼šæ£€æµ‹å‚æ•°æ˜¯å¦æœ‰NaN
                has_nan = False
                for name, param in self.policy.named_parameters():
                    if torch.isnan(param).any():
                        print(f"âš ï¸ ä¸¥é‡é”™è¯¯: å‚æ•° {name} åŒ…å«NaNï¼")
                        has_nan = True
                
                if has_nan:
                    print("ğŸ›‘ è®­ç»ƒä¸­æ­¢ï¼šå‚æ•°å‡ºç°NaNï¼Œè¯·é™ä½å­¦ä¹ ç‡æˆ–æ£€æŸ¥æ•°æ®")
                    raise ValueError("å‚æ•°åŒ…å«NaNï¼Œè®­ç»ƒå¤±è´¥")
                
                # ç»Ÿè®¡
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                total_entropy += entropy.mean().item()
                n_updates += 1
        
        # æ¸…ç©ºbuffer
        rollout_buffer.reset()
        
        return {
            'policy_loss': total_policy_loss / n_updates,
            'value_loss': total_value_loss / n_updates,
            'entropy': total_entropy / n_updates,
        }
    
    def save(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy': self.policy.state_dict(),
            'optimizer': self.optimizer.state_dict(),
        }, path)
    
    def load(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.policy.load_state_dict(checkpoint['policy'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
    
    def load_bc_pretrain(self, checkpoint_path):
        """ä»BCé¢„è®­ç»ƒåŠ è½½actorå‚æ•°ï¼ˆå¸¦numpyå…¼å®¹æ€§å¤„ç†ï¼‰"""
        print(f"\nåŠ è½½BCé¢„è®­ç»ƒæ¨¡å‹: {checkpoint_path}")
        
        # ä¿®å¤numpyå…¼å®¹æ€§
        if not hasattr(np, '_core'):
            import numpy.core as _core
            sys.modules['numpy._core'] = _core
            sys.modules['numpy._core.multiarray'] = _core.multiarray
            sys.modules['numpy._core.umath'] = _core.umath
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        except ModuleNotFoundError as e:
            if 'numpy._core' in str(e):
                import pickle
                class NumpyCompatUnpickler(pickle.Unpickler):
                    def find_class(self, module, name):
                        if module.startswith('numpy._core'):
                            module = module.replace('numpy._core', 'numpy.core')
                        return super().find_class(module, name)
                
                with open(checkpoint_path, 'rb') as f:
                    checkpoint = NumpyCompatUnpickler(f).load()
            else:
                raise
        
        try:
            # å°è¯•åŠ è½½åŒ¹é…çš„æƒé‡
            self.policy.load_state_dict(checkpoint['model_state_dict'], strict=False)
            print("  âœ“ BCé¢„è®­ç»ƒæƒé‡å·²åŠ è½½")
        except Exception as e:
            print(f"  âš  æ— æ³•åŠ è½½BCæƒé‡: {e}")
            print("  â†’ ä»å¤´å¼€å§‹è®­ç»ƒ")


# ============================================================================
# ä¸»è®­ç»ƒå¾ªç¯
# ============================================================================

def extract_reward_components(env):
    """
    ä»Isaac Labç¯å¢ƒçš„reward_managerä¸­æå–å„ä¸ªå¥–åŠ±é¡¹çš„å€¼
    
    Returns:
        dict: å¥–åŠ±é¡¹åç§° -> å¹³å‡å€¼çš„å­—å…¸
    """
    reward_dict = {}
    
    # é€šè¿‡reward_managerè·å–å„ä¸ªå¥–åŠ±é¡¹
    try:
        # ManagerBasedRLEnvæ²¡æœ‰unwrappedï¼Œç›´æ¥è®¿é—®
        if hasattr(env, 'reward_manager'):
            manager = env.reward_manager
            # ä½¿ç”¨_episode_sumsè€Œä¸æ˜¯_term_buffers
            if hasattr(manager, '_episode_sums'):
                for term_name, term_buffer in manager._episode_sums.items():
                    if isinstance(term_buffer, torch.Tensor):
                        # å–å½“å‰æ­¥çš„å€¼ï¼ˆä¸æ˜¯episodeç´¯ç§¯å’Œï¼‰
                        reward_dict[term_name] = term_buffer.mean().item()
    except Exception as e:
        print(f"âš ï¸ extract_reward_componentsé”™è¯¯: {e}")
    
    return reward_dict


def train():
    """ä¸»è®­ç»ƒå‡½æ•°"""

    # æˆåŠŸåˆ¤å®šé˜ˆå€¼ï¼ˆä¸ç¯å¢ƒç»ˆæ­¢æ¡ä»¶ goal_reached_termination ä¿æŒä¸€è‡´ï¼‰
    # ç›®å‰åœ¨ scripts/rosorin_env_cfg.py / configs/mdp/rosorin_mdp.py ä¸­é»˜è®¤ä¹Ÿæ˜¯ 0.5mã€‚
    success_distance_threshold = 0.5
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒåˆ›å»ºä¸åŒçš„ç›®å½•
    if args.pretrain_checkpoint:
        run_dir = output_dir / f"ppo_with_bc_{timestamp}"
    else:
        run_dir = output_dir / f"ppo_scratch_{timestamp}"
    
    run_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint_dir = run_dir / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config = vars(args)
    with open(run_dir / "config.yaml", 'w') as f:
        yaml.dump(config, f)
    
    # åˆ›å»ºç¯å¢ƒ
    print("\nåˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    print("  ä½¿ç”¨PPOä¸“ç”¨å¥–åŠ±é…ç½®...")
    env_cfg = create_ppo_env_cfg(num_envs=args.num_envs)
    env = ManagerBasedRLEnv(cfg=env_cfg)
    
    # è·å–ç»´åº¦
    obs_dict, _ = env.reset()
    obs_dim = obs_dict["policy"].shape[-1]
    action_dim = env.action_space.shape[-1]
    print(f"  è§‚å¯Ÿç»´åº¦: {obs_dim}")
    print(f"  åŠ¨ä½œç»´åº¦: {action_dim}")

    # è§£æç›¸æœºå½¢çŠ¶ï¼Œå¹¶æ®æ­¤åˆ‡åˆ†obsä¸ºï¼šä½ç»´çŠ¶æ€ + RGB(flat) + Depth(flat)
    try:
        rgb_out = env.scene.sensors["camera"].data.output["rgb"]
        depth_out = env.scene.sensors["camera"].data.output["distance_to_image_plane"]
        rgb_h, rgb_w, rgb_c = int(rgb_out.shape[1]), int(rgb_out.shape[2]), int(rgb_out.shape[3])
        if depth_out.ndim == 4:
            dep_h, dep_w, dep_c = int(depth_out.shape[1]), int(depth_out.shape[2]), int(depth_out.shape[3])
        else:
            dep_h, dep_w, dep_c = int(depth_out.shape[1]), int(depth_out.shape[2]), 1
        rgb_dim = rgb_h * rgb_w * rgb_c
        depth_dim = dep_h * dep_w * dep_c
        low_dim = int(obs_dim - rgb_dim - depth_dim)
        if low_dim <= 0:
            raise ValueError(f"invalid low_dim={low_dim} (obs_dim={obs_dim}, rgb_dim={rgb_dim}, depth_dim={depth_dim})")
    except Exception as e:
        raise RuntimeError(f"Failed to infer camera shapes for CNN policy: {e}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºPPOæ™ºèƒ½ä½“
    print("\nåˆå§‹åŒ–PPOæ™ºèƒ½ä½“...")
    agent = PPOAgent(
        obs_dim,
        action_dim,
        device,
        low_dim=low_dim,
        rgb_dim=rgb_dim,
        depth_dim=depth_dim,
        rgb_shape=(rgb_h, rgb_w, rgb_c),
        depth_shape=(dep_h, dep_w, dep_c),
        lr=args.lr,
        clip_range=args.clip_range,
        vf_coef=args.vf_coef,
        ent_coef=args.ent_coef,
    )
    
    # âš ï¸ BCé¢„è®­ç»ƒä¸PPOä¸å…¼å®¹ï¼šBCè¾“å‡ºtanhåçš„åŠ¨ä½œï¼ŒPPOéœ€è¦åŸå§‹é«˜æ–¯åŠ¨ä½œ
    # å¼ºåˆ¶ä»å¤´è®­ç»ƒä»¥é¿å…NaNå´©æºƒ
    if args.pretrain_checkpoint:
        print(f"\nâš ï¸ è­¦å‘Š: BCé¢„è®­ç»ƒä¸PPOä¸å…¼å®¹ï¼ˆåŠ¨ä½œåˆ†å¸ƒä¸åŒï¼‰ï¼Œå°†ä»å¤´è®­ç»ƒ")
        print("  BCè¾“å‡º: tanhåçš„åŠ¨ä½œ âˆˆ [-1,1]")
        print("  PPOéœ€è¦: é«˜æ–¯åˆ†å¸ƒåŸå§‹åŠ¨ä½œ âˆˆ â„")
        print("  â†’ ä»å¤´è®­ç»ƒPPOï¼Œä¸åŠ è½½BCæƒé‡\n")
    # if args.pretrain_checkpoint and Path(args.pretrain_checkpoint).exists():
    #     agent.load_bc_pretrain(args.pretrain_checkpoint)
    
    # ğŸ†• å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆçº¿æ€§è¡°å‡ï¼Œé¿å…åæœŸä¸ç¨³å®šï¼‰
    initial_lr = args.lr
    def lr_schedule(step):
        """çº¿æ€§è¡°å‡åˆ°åˆå§‹å­¦ä¹ ç‡çš„10%"""
        progress = step / args.total_steps
        return max(0.1, 1.0 - 0.9 * progress)
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(agent.optimizer, lr_schedule)
    
    # åˆ›å»ºRollout Buffer
    print("\nåˆ›å»ºRollout Buffer...")
    rollout_buffer = RolloutBuffer(
        buffer_size=args.n_steps,
        obs_dim=obs_dim,
        action_dim=action_dim,
        num_envs=args.num_envs,
        device=device
    )
    
    # è®­ç»ƒç»Ÿè®¡
    total_steps = 0
    episode_rewards = []
    episode_lengths = []
    episode_successes = []  # è®°å½•æˆåŠŸ/å¤±è´¥
    best_moving_avg_reward = 0.0
    max_episode_reward = -float('inf')
    max_episode_length = 0
    
    # ç¯å¢ƒçŠ¶æ€
    obs = obs_dict["policy"]
    obs = torch.nan_to_num(obs, nan=0.0, posinf=10.0, neginf=0.0)
    
    episode_reward = torch.zeros(args.num_envs, device=device)
    episode_length = torch.zeros(args.num_envs, device=device, dtype=torch.int)
    
    # å¥–åŠ±é¡¹ç»Ÿè®¡
    reward_components = {
        'progress': [],
        'goal_reached': [],
        'velocity': [],
        'orientation': [],
        'obstacle_avoidance': [],  # ğŸ†• é¿éšœ
        'smooth_action': [],
        'collision': [],
        'stability': [],
        'height': [],
    }
    
    # æ—¥å¿—æ•°æ®
    log_data = {
        'steps': [],
        'rewards': [],
        'policy_loss': [],
        'value_loss': [],
        'entropy': [],
        'reward_components': [],
        # Debug metric from depth camera (set inside obstacle_avoidance_penalty)
        'front_min_depth': [],
        # Navigation diagnostics
        'goal_distance_mean': [],
        'vel_toward_goal_mean': [],
    }
    
    print(f"\n{'='*80}")
    if args.pretrain_checkpoint:
        print("å¼€å§‹è®­ç»ƒPPO (ä½¿ç”¨BCé¢„è®­ç»ƒ)...")
    else:
        print("å¼€å§‹è®­ç»ƒPPO (ä»å¤´å¼€å§‹)...")
    print(f"{'='*80}\n")
    
    pbar = tqdm(total=args.total_steps, desc="è®­ç»ƒè¿›åº¦")
    
    while total_steps < args.total_steps:
        # Rollouté˜¶æ®µ
        for _ in range(args.n_steps):
            # é€‰æ‹©åŠ¨ä½œ
            with torch.no_grad():
                actions, values, log_probs = agent.select_action(obs, deterministic=False)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs_dict, rewards, terminated, truncated, infos = env.step(actions)
            next_obs = next_obs_dict["policy"]
            
            # âš ï¸ æ•°å€¼ç¨³å®šæ€§ï¼šæ£€æŸ¥è§‚æµ‹å’Œå¥–åŠ±
            next_obs = torch.nan_to_num(next_obs, nan=0.0, posinf=10.0, neginf=-10.0)
            rewards = torch.nan_to_num(rewards, nan=0.0, posinf=100.0, neginf=-100.0)
            rewards = torch.clamp(rewards, min=-100.0, max=200.0)
            
            # æ£€æµ‹æ˜¯å¦æœ‰å¼‚å¸¸å¥–åŠ±
            if torch.isnan(rewards).any() or torch.isinf(rewards).any():
                print(f"âš ï¸ è­¦å‘Š: ç¬¬{total_steps}æ­¥å¥–åŠ±åŒ…å«NaN/Infï¼Œå·²ä¿®å¤")
                rewards = torch.nan_to_num(rewards, nan=0.0, posinf=100.0, neginf=-100.0)
            

            # æå–å½“å‰æ­¥çš„å¥–åŠ±ç»†èŠ‚ï¼ˆåœ¨stepä¹‹åç«‹å³æå–ï¼‰
            current_reward_components = extract_reward_components(env)

            # ğŸ†• æ·±åº¦ç›¸æœºè°ƒè¯•æŒ‡æ ‡ï¼šå‰æ–¹ä¸­å¿ƒåŒºåŸŸæœ€å°æ·±åº¦ï¼ˆç±³ï¼‰
            front_min_depth_mean = None
            try:
                if hasattr(env, 'last_front_min_depth') and isinstance(env.last_front_min_depth, torch.Tensor):
                    d = torch.nan_to_num(env.last_front_min_depth.float(), nan=10.0, posinf=10.0, neginf=0.0)
                    front_min_depth_mean = d.mean().item()
            except Exception:
                front_min_depth_mean = None

            # ğŸ†• å¯¼èˆªè¯Šæ–­ï¼šåˆ°ç›®æ ‡è·ç¦»å‡å€¼ã€æœç›®æ ‡é€Ÿåº¦æŠ•å½±å‡å€¼
            goal_distance_mean = None
            vel_toward_goal_mean = None
            try:
                if hasattr(env, 'goal_positions'):
                    robot_pos = env.scene.articulations["robot"].data.root_pos_w[:, :2]
                    goal_pos = env.goal_positions[:, :2]
                    to_goal = goal_pos - robot_pos
                    goal_distance = torch.norm(to_goal, dim=-1)
                    goal_distance_mean = goal_distance.mean().item()

                    lin_vel_w = env.scene.articulations["robot"].data.root_lin_vel_w[:, :2]
                    to_goal_norm = torch.norm(to_goal, dim=-1, keepdim=True)
                    to_goal_dir = to_goal / (to_goal_norm + 1e-6)
                    vel_toward_goal = torch.sum(lin_vel_w * to_goal_dir, dim=-1)
                    vel_toward_goal_mean = vel_toward_goal.mean().item()
            except Exception:
                goal_distance_mean = None
                vel_toward_goal_mean = None
            
            # ğŸ†• è°ƒè¯•ï¼šæ¯1000æ­¥æ‰“å°ä¸€æ¬¡å¥–åŠ±ç»†èŠ‚
            if total_steps % 1000 == 0 and total_steps > 0:
                print(f"\n[è°ƒè¯• Step {total_steps}]")
                print(f"  å½“å‰æ­¥å¥–åŠ±: {rewards.mean().item():.3f} (èŒƒå›´: [{rewards.min().item():.3f}, {rewards.max().item():.3f}])")
                print(f"  Episode: {len(episode_rewards)}ä¸ªå®Œæˆ, å½“å‰é•¿åº¦: {episode_length.float().mean().item():.0f}")
                if front_min_depth_mean is not None:
                    print(f"  å‰æ–¹æœ€å°æ·±åº¦(æ¥è‡ªDepthç›¸æœº): {front_min_depth_mean:.3f} m")
                if current_reward_components:
                    # è®¡ç®—å•æ­¥å¥–åŠ±ï¼ˆé™¤ä»¥å½“å‰episodeé•¿åº¦ï¼‰
                    avg_ep_len = max(episode_length.float().mean().item(), 1)
                    print(f"  å•æ­¥å¥–åŠ±ç»„ä»¶ (ç´¯ç§¯å€¼/{avg_ep_len:.0f}æ­¥):")
                    for key, val in sorted(current_reward_components.items()):
                        step_reward = val / avg_ep_len
                        print(f"    {key:20s}: {step_reward:+.4f}")
                else:
                    print(f"  âš ï¸ å¥–åŠ±ç»„ä»¶æå–å¤±è´¥")


            
            done_flags = (terminated | truncated)
            dones = done_flags.float().unsqueeze(-1)

            # ä¸¥æ ¼æˆåŠŸåˆ¤å®šï¼šepisodeç»“æŸæ—¶è‹¥è·ç¦»ç›®æ ‡ < é˜ˆå€¼ï¼Œåˆ™è®°ä¸ºsuccess
            success_flags = None
            try:
                if hasattr(env, 'goal_positions'):
                    robot_pos_xy = env.scene.articulations["robot"].data.root_pos_w[:, :2]
                    goal_pos_xy = env.goal_positions[:, :2]
                    goal_dist = torch.norm(robot_pos_xy - goal_pos_xy, dim=-1)
                    success_flags = goal_dist < success_distance_threshold
            except Exception:
                success_flags = None
            
            # å­˜å‚¨åˆ°buffer
            rollout_buffer.add(obs, actions, rewards.unsqueeze(-1), values, log_probs, dones)
            
            # æ›´æ–°ç»Ÿè®¡
            for i in range(args.num_envs):
                episode_reward[i] += rewards[i]
                episode_length[i] += 1

                if bool(done_flags[i].item()):
                    episode_rewards.append(episode_reward[i].item())
                    episode_lengths.append(episode_length[i].item())

                    # è®°å½•å…¨å±€æœ€å¤§episodeä¿¡æ¯ï¼ˆä¾¿äºè§£é‡Šâ€œå¹³å‡å¥–åŠ±/æœ€ä½³å¥–åŠ±â€å£å¾„å·®å¼‚ï¼‰
                    if episode_reward[i].item() > max_episode_reward:
                        max_episode_reward = episode_reward[i].item()
                    if episode_length[i].item() > max_episode_length:
                        max_episode_length = int(episode_length[i].item())
                    
                    # ä¿å­˜å½“å‰episodeçš„å¥–åŠ±ç»„ä»¶ï¼ˆé™¤ä»¥episodeé•¿åº¦è·å¾—å¹³å‡å€¼ï¼‰
                    ep_len = episode_length[i].item()
                    for key, value in current_reward_components.items():
                        if key in reward_components and ep_len > 0:
                            reward_components[key].append(value / ep_len)  # å¹³å‡å•æ­¥å¥–åŠ±
                    
                    # è®°å½•æˆåŠŸç‡ï¼ˆä¸¥æ ¼ï¼šç»ˆæ­¢æ—¶è·ç¦» < distance_thresholdï¼‰
                    if success_flags is not None and bool(success_flags[i].item()):
                        episode_successes.append(1)
                    else:
                        episode_successes.append(0)
                    
                    # é‡ç½®episodeç»Ÿè®¡
                    episode_reward[i] = 0
                    episode_length[i] = 0
            
            obs = next_obs
            total_steps += args.num_envs
            pbar.update(args.num_envs)
            
            if total_steps >= args.total_steps:
                break
        
        # æ›´æ–°ç­–ç•¥
        metrics = agent.update(rollout_buffer, args.n_epochs, args.batch_size)
        
        # ğŸ†• æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = agent.optimizer.param_groups[0]['lr']
        
        # è®°å½•æ—¥å¿—
        if total_steps % args.log_freq == 0:
            avg_reward = np.mean(episode_rewards[-20:]) if episode_rewards else 0.0
            avg_length = np.mean(episode_lengths[-20:]) if episode_lengths else 0.0

            # å½“å‰æ­£åœ¨è¿›è¡Œçš„episodeç»Ÿè®¡ï¼ˆè·¨envå¹³å‡ï¼‰ï¼Œç”¨äºçŸ­è·‘/episodeå¾ˆå°‘å®Œæˆæ—¶çš„å¯è§£é‡Šæ€§
            running_reward_mean = float(episode_reward.mean().item())
            running_length_mean = float(episode_length.float().mean().item())
            
            # è®¡ç®—å¹³å‡å¥–åŠ±ç»„ä»¶
            comp_str = {}
            for key, values in reward_components.items():
                if values:
                    avg_val = np.mean(values[-20:])
                    comp_str[key[:3]] = f"{avg_val:.2f}"
            
            pbar.set_postfix({
                'reward': f"{avg_reward:.2f}",
                'len': f"{avg_length:.0f}",
                'runR': f"{running_reward_mean:.2f}",
                'runL': f"{running_length_mean:.0f}",
                'policy': f"{metrics['policy_loss']:.3f}",
                'value': f"{metrics['value_loss']:.3f}",
                'lr': f"{current_lr:.2e}",  # ğŸ†• æ˜¾ç¤ºå½“å‰å­¦ä¹ ç‡
                'dmin': f"{front_min_depth_mean:.2f}" if front_min_depth_mean is not None else "NA",
                'gdist': f"{goal_distance_mean:.2f}" if goal_distance_mean is not None else "NA",
                'vtg': f"{vel_toward_goal_mean:.2f}" if vel_toward_goal_mean is not None else "NA",
                **comp_str,
            })
            
            log_data['steps'].append(total_steps)
            log_data['rewards'].append(avg_reward)
            log_data['policy_loss'].append(metrics['policy_loss'])
            log_data['value_loss'].append(metrics['value_loss'])
            log_data['entropy'].append(metrics['entropy'])

            # è®°å½•æ·±åº¦è°ƒè¯•æŒ‡æ ‡
            log_data['front_min_depth'].append(float(front_min_depth_mean) if front_min_depth_mean is not None else None)

            # è®°å½•å¯¼èˆªè¯Šæ–­æŒ‡æ ‡
            log_data['goal_distance_mean'].append(float(goal_distance_mean) if goal_distance_mean is not None else None)
            log_data['vel_toward_goal_mean'].append(float(vel_toward_goal_mean) if vel_toward_goal_mean is not None else None)
            
            # ä¿å­˜å¥–åŠ±ç»„ä»¶
            comp_avg = {k: float(np.mean(v[-20:])) if v else 0.0 
                       for k, v in reward_components.items()}
            log_data['reward_components'].append(comp_avg)
            
            # æ¯1000æ­¥æ‰“å°è¯¦ç»†ä¿¡æ¯
            if total_steps % 1000 == 0:
                print(f"\n[Step {total_steps:,}] å¥–åŠ±ç»†èŠ‚:")
                print(f"  æ€»å¥–åŠ±: {avg_reward:.2f} | Episodeé•¿åº¦: {avg_length:.0f}")
                print(f"  è¿›åº¦: {comp_avg.get('progress', 0):.4f} | åˆ°è¾¾: {comp_avg.get('goal_reached', 0):.4f}")
                print(f"  é€Ÿåº¦: {comp_avg.get('velocity', 0):.4f} | æœå‘: {comp_avg.get('orientation', 0):.4f}")
                print(f"  å¹³æ»‘: {comp_avg.get('smooth_action', 0):.4f} | ç¢°æ’: {comp_avg.get('collision', 0):.4f}")
                if goal_distance_mean is not None and vel_toward_goal_mean is not None:
                    print(f"  ç›®æ ‡è·ç¦»å‡å€¼: {goal_distance_mean:.3f} m | æœç›®æ ‡é€Ÿåº¦: {vel_toward_goal_mean:.3f} m/s")
        
        # ä¿å­˜checkpoint
        if total_steps % args.save_freq == 0 and total_steps > 0:
            checkpoint_path = checkpoint_dir / f"checkpoint_{total_steps}.pt"
            agent.save(checkpoint_path)
            
            # ä¿å­˜è®­ç»ƒæ—¥å¿—
            with open(run_dir / "training_log.json", 'w') as f:
                json.dump(log_data, f, indent=2)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if episode_rewards and np.mean(episode_rewards[-20:]) > best_moving_avg_reward:
                best_moving_avg_reward = np.mean(episode_rewards[-20:])
                best_path = checkpoint_dir / "best_model.pt"
                agent.save(best_path)
                print(f"\nâœ“ æ–°çš„æœ€ä½³æ¨¡å‹ (æœ€è¿‘20å›åˆå¹³å‡å¥–åŠ±: {best_moving_avg_reward:.2f})")
    
    pbar.close()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = checkpoint_dir / "final_model.pt"
    agent.save(final_path)
    
    # ä¿å­˜è®­ç»ƒæ‘˜è¦
    summary = {
        'total_steps': total_steps,
        'total_episodes': len(episode_rewards),
        'mean_reward': float(np.mean(episode_rewards)) if episode_rewards else 0.0,
        'std_reward': float(np.std(episode_rewards)) if episode_rewards else 0.0,
        # è¯´æ˜ï¼šè¿™æ˜¯â€œæœ€è¿‘20å›åˆå‡å€¼â€çš„æœ€ä½³å€¼ï¼Œä¸ç­‰åŒäºå•å›åˆæœ€å¤§å€¼
        'best_moving_avg_reward_20': float(best_moving_avg_reward),
        'max_episode_reward': float(max_episode_reward) if episode_rewards else 0.0,
        'max_episode_length': int(max_episode_length),
        'mean_length': float(np.mean(episode_lengths)) if episode_lengths else 0.0,
        'success_rate': float(np.mean(episode_successes)) if episode_successes else 0.0,
        'success_distance_threshold': float(success_distance_threshold),
        'with_bc_pretrain': args.pretrain_checkpoint is not None,
    }
    
    with open(run_dir / "summary.json", 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\n{'='*80}")
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*80}")
    print(f"æ€»æ­¥æ•°: {total_steps:,}")
    print(f"æ€»Episodes: {len(episode_rewards)}")
    print(f"å¹³å‡å¥–åŠ±: {summary['mean_reward']:.2f} Â± {summary['std_reward']:.2f}")
    print(f"æœ€ä½³(æœ€è¿‘20å›åˆå‡å€¼): {best_moving_avg_reward:.2f}")
    print(f"å•å›åˆæœ€é«˜å¥–åŠ±: {summary['max_episode_reward']:.2f} | å•å›åˆæœ€é•¿é•¿åº¦: {summary['max_episode_length']}")
    print(f"æ¨¡å‹ä¿å­˜ä½ç½®: {run_dir}")
    print(f"{'='*80}\n")
    
    # å…³é—­ç¯å¢ƒ
    env.close()
    simulation_app.close()


if __name__ == "__main__":
    train()
